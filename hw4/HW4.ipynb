{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision:  HW 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Science: COMS W 4995 005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due: October 31, 2024\n",
    "\n",
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In this notebook we provide three networks for classifying handwritten digits from the MNIST dataset. The networks are implemented and tested using the Tensorflow framework. The third and final network is a convolutional neural network (CNN aka ConvNet) which achieves 99.18% accuracy on this dataset. \n",
    "\n",
    "    Your task is to re-implement all three networks using Pytorch. You will likely find several Pytorch implementations on the internet. It is ok to study these. However, you must not cut and paste this code into your assignment--you must write this yourself. Furthermore, you need to comment every line of code and succintly explain what it is doing! The MNIST dataset should be loadable from PyTorch/torchvision and part of your assignment is to find this and load it yourself.\n",
    "\n",
    "    Here is what is required:\n",
    "\n",
    "    a) A FULLY commented re-implementation of the networks below using Pytorch.\n",
    "\n",
    "    b) your network trained on the same MNIST data as used here.\n",
    "\n",
    "    c) an evaluation of the accuracy on the MNIST test set.\n",
    "\n",
    "    d) plots of 10 randomly selected digits from the test set along with the correct label and the assigned label.\n",
    "\n",
    "    e) have your training record a log of the validation loss and validation accuracy. \n",
    "\n",
    "    f) have your training continually save the best model so far (as determined by the validation loss).\n",
    "\n",
    "    g) after training, load the saved weights using the best model so far. re-run you accuracy evaluation using these saved weights.\n",
    "\n",
    "    Below we include the Tensorflow examples shown in class.  \n",
    "   <p>&nbsp;</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Convolutional Neural Network in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers a python and tensorflow-based solution to the handwritten digits recognition problem. It is based on tensorflow tutorials and Yann LeCun's early work on CNN's. This toturial compares a simple softmax regressor, a multi-layer perceptron (MLP), and a simple convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the MNIST digit dataset directly from tensorflow examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rosh\\scoop\\apps\\python\\current\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rosh\\scoop\\apps\\python\\current\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\rosh\\scoop\\apps\\python\\current\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\rosh\\scoop\\apps\\python\\current\\lib\\site-packages (2.18.0)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy matplotlib pandas tensorflow sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtutorials\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_data\n\u001b[0;32m      2\u001b[0m mnist \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mread_data_sets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNIST_data\u001b[39m\u001b[38;5;124m'\u001b[39m, one_hot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_worker_test_base\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m def_function\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\distribute\\multi_worker_test_base.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster_resolver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster_resolver \u001b[38;5;28;01mas\u001b[39;00m cluster_resolver_lib\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitored_session\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m server_lib\n\u001b[0;32m     33\u001b[0m _thread_local \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variables\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m basic_session_run_hooks\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordinator\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\summary\\summary.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tb_summary\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# exports FileWriter, FileWriterCache\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter_cache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriterCache\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# pylint: enable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plugin_asset\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriterV2\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\tensorflow\\python\\summary\\writer\\event_file_writer.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_events_writer\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import tensorflow and begin an interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'InteractiveSession'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveSession\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'InteractiveSession'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression Model on the MNIST Digits Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create placeholders for the data. Data will be dumped here when it is batched from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what this data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mmnist\u001b[49m\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mnext_batch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(batch[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n\u001b[0;32m      7\u001b[0m     label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(4):\n",
    "    batch = mnist.test.next_batch(1)\n",
    "    image = np.asarray(batch[0]).reshape((28, 28))\n",
    "    label = batch[1]\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first going to do softmax logistic regression. This is a linear layer followed by softmax. Note there are NO hidden layers here. Also note that the digit images (28x28 grayscale images) are reshaped into a 784 element vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create the parameters (weights) for our linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use tensorflows initializer to initialize these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our linear layer as a function of the input and the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_regressor = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create our loss function. Note that the cross entropy is $ H_{\\hat{y}}(y) = -\\sum_i \\hat{y}_{i} \\, \\log(y_{i})$ where $\\hat{y}$ is the true probability distribution and is expressed as a one-hot vector, $y$ is the estimated probability distribution, and $i$ indexes elements of these two vectors. Also note that this reduces to $ H_{\\hat{y}}(y) = -\\, \\log(y_{i^*})$ where $i^*$ is the correct label. And if we sum this over all of our samples indexed by $j$, then $H_{\\hat{y}}(y) = -\\sum_j  \\log(y^{(j)}_{i^*})$. This is precisely the same loss function as we used before, but we called the MLE loss. They are one and the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_regressor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tell tf to use gradient descent with a step size of 0.5 and to minimize the cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train by grabbing mini-batches with 100 samples each and pushing these through the network to update our weights (W and b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch = mnist.train.next_batch(100)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define how to compute correct predicitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_regressor,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from these correct predictions how to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out some test images and the corresponsing predictions made by the network. But first, let's add an output to the computation graph that computes the softmax probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_regressor = tf.nn.softmax(logits=y_regressor, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAABiZJREFUeJzt3UuolHUcxvF35ngsS9TKVMLCQMgoLYooKyIIN12ICrtsW7QIiXYRbSJw1aJVkEV0gSSyICqCkhZBJUTkhcDSzAwvkFmUyjH1zLRwU9D7G/V4RvH5fLbPeT3j4ut/8Z8ZO/1+vwHydE/3CwBOD/FDKPFDKPFDKPFDKPFDKPFDKPFDKPFDqCnD/GXLusu9nRAm2drems7x/JyTH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0KJH0JNOd0vgKbpXH9Vue+4Z2a533bn+tZt1fx15bPj/V65L93wULlf/NjBcj+6a3e5c/o4+SGU+CGU+CGU+CGU+CGU+CGUq74hGHSVt/Ld18p9ydSRk/7d246Mlfvag1eU+5fXvl3uN929otxnr3LVd6Zy8kMo8UMo8UMo8UMo8UMo8UMo8UMo9/xDMOgjuYPu8R/48a5yH3tmXus25a9D5bPN9l3l/N41y8p97t695X506TWtW2fdxvJZJpeTH0KJH0KJH0KJH0KJH0KJH0KJH0K55x+CkSV/Tuj56h6/aZqm+8WG1q3+Yu7Bqj+7aZpmfMDzvz5xc+s2t/5WcSaZkx9CiR9CiR9CiR9CiR9CiR9CiR9Cuecfgk6nX+6bjxwp99G9B8p90F07/B8nP4QSP4QSP4QSP4QSP4QSP4QSP4Ryzz8EB/44r9yvHB0t96kv7y/3w/df2LqN7/u9fHaQHc8tLfdH7v283GePfti6rb7zhvLZ/Z/W32Mw/43vy32if/eznZMfQokfQokfQokfQokfQokfQrnqG4JLP6j/jd15x1i5r1n4cbm/89Wc1m3H37PLZ3tNp9xfn/V8uc8emVbu3eLPf2zxz+WzzeJ6vvW3FeU+603fDV5x8kMo8UMo8UMo8UMo8UMo8UMo8UMo9/xDMO39r8v90bEny/2Fl14s9wen/9q6dafvLZ/tNfXXijdNfY8/yOO7bmndnp23tnx20HsIDl1Qv0eBmpMfQokfQokfQokfQokfQokfQokfQrnnPwNM/eSbcn/q8hvLvbtkUeu25/b2r/U+HlPG6vcBXPTKoM/Mt39XwdafppdPzhnplfv0PfVOzckPocQPocQPocQPocQPocQPocQPodzznwV6m9r/q+q5m4b4Qv7HyEXt7zM4v3O4fLbXjJzql8O/OPkhlPghlPghlPghlPghlPghlPghlHt+JtXRRZe1bpeM1Pf86w+fU+4zP9tS7uPlipMfQokfQokfQokfQokfQokfQrnqY1IdmH9u6zbov+D+4chouY/v+/2kXhPHOPkhlPghlPghlPghlPghlPghlPghlHt+JtX+h/9q3bpNp3z26S33lfuMZttJvSaOcfJDKPFDKPFDKPFDKPFDKPFDKPFDKPf8TKp+v/0uv9f0y2d/2zin3N3zT4yTH0KJH0KJH0KJH0KJH0KJH0KJH0K552dSrb7u1WKtv5d/wUdjp/bF8B9Ofgglfgglfgglfgglfgglfgjlqo8J6S5ZVO5XjX7bur21v/7I7ujmX8p9vFwZxMkPocQPocQPocQPocQPocQPocQPodzzMyFbn5pW7iOd9vNl5Zrl5bML9q07qdfE8XHyQyjxQyjxQyjxQyjxQyjxQyjxQyj3/EzI1fN3l/t4vzekV8KJcvJDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDKJ/nZ0K+23lJ/QML26cZ20/ta+HEOPkhlPghlPghlPghlPghlPghVKff7w/tly3rLh/eL4NQa3trOsfzc05+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CCV+CDXUz/MDZw4nP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4QSP4T6ByeGuap49xCaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e261f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "Class probabilities =  [[  9.29443468e-06   7.47387749e-05   1.38089963e-05   1.14219487e-01\n",
      "    3.84134240e-02   6.48420528e-02   1.38886957e-04   1.47537813e-02\n",
      "    1.18470658e-02   7.55687416e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0a1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "Class probabilities =  [[  8.50503985e-03   4.94107439e-07   4.09405155e-04   6.29698661e-06\n",
      "    7.67807290e-02   2.39031506e-04   2.94614192e-02   1.68453693e-01\n",
      "    8.35071690e-03   7.07793236e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e157cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "Class probabilities =  [[  1.15769981e-05   2.45307339e-04   1.97686313e-05   6.34711876e-04\n",
      "    5.48726879e-04   2.86388420e-03   1.54315749e-05   9.59967077e-01\n",
      "    4.33111069e-04   3.52605022e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0cf0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "Class probabilities =  [[  3.59805243e-04   6.58656973e-06   5.26350783e-03   8.48718992e-05\n",
      "    9.36258510e-02   1.88619215e-05   8.44072938e-01   2.85940617e-02\n",
      "    5.35306940e-03   2.26204991e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11df2a2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "Class probabilities =  [[  1.30033266e-04   3.87917506e-03   1.01224063e-02   2.52482090e-02\n",
      "    1.44047150e-02   4.26409282e-02   1.45911865e-04   1.89911807e-03\n",
      "    8.69454384e-01   3.20750922e-02]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = mnist.test.next_batch(1)\n",
    "    image = np.asarray(batch[0]).reshape((28, 28))\n",
    "    label = batch[1]\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print (\"Label = \", label)\n",
    "    print (\"Class probabilities = \", y_probs_regressor.eval(feed_dict={\n",
    "        x: batch[0], y_: batch[1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Multi-Layer Perceptron on the MNIST Digits Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define both weight and bias variables and how they are to be initialized. Note that the weights are distributed according to a standard normal distribution (mean = 0, std = 0.1). This random initialization helps avoid hidden units get stuck together, as units that start with the same value will be updated identically in the non-convolutional layers. In contrast, the bias variables are set to a small positive number--this is help prevent hidden units from starting out and getting stuck in the zero part of the ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create placeholders for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the first and only fully connected hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h = weight_variable([784, 512])\n",
    "b_h = bias_variable([512])\n",
    "h = tf.nn.relu(tf.matmul(x, W_h) + b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_out = weight_variable([512, 10])\n",
    "b_out = bias_variable([10])\n",
    "y_MLP = tf.matmul(h, W_out) + b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use cross entropy loss on a softmax distribution on the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_MLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we choose an Adam learning rate and update rule. We then run this for 20,000 iterations and evaluate our accuracy after training. Note this softmax MLP network does quite a bit bettter than our softmax regressor. The non-linear layer really helps makes sense of the data! But we can do better still..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.16\n",
      "step 1000, training accuracy 0.88\n",
      "step 2000, training accuracy 0.94\n",
      "step 3000, training accuracy 0.86\n",
      "step 4000, training accuracy 1\n",
      "step 5000, training accuracy 0.96\n",
      "step 6000, training accuracy 1\n",
      "step 7000, training accuracy 0.98\n",
      "step 8000, training accuracy 0.96\n",
      "step 9000, training accuracy 0.98\n",
      "step 10000, training accuracy 1\n",
      "step 11000, training accuracy 0.98\n",
      "step 12000, training accuracy 0.98\n",
      "step 13000, training accuracy 0.98\n",
      "step 14000, training accuracy 0.98\n",
      "step 15000, training accuracy 1\n",
      "step 16000, training accuracy 0.98\n",
      "step 17000, training accuracy 1\n",
      "step 18000, training accuracy 0.98\n",
      "step 19000, training accuracy 1\n",
      "test accuracy 0.978\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_MLP,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%1000 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1]})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Convolutional Neural Network: LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make our first CNN. It's quite simple network, but it's surprisingly good at this handwritten digit recognition task. This a variant on Yann LeCun's CNN network that really helped to move deep learning forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define both weight and bias variables and how they are to be initialized. Note that the weights are are distributed according to a standard normal distribution (mean = 0, std = 0.1). This random initialization helps avoid hidden units get stuck together, as units that start with the same value will be updated identically in the non-convolutional layers. In contrast, the bias variables are set to a small positive number--this is help prevent hidden units from starting out and getting stuck in the zero part of the ReLu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define how the convolution is to be computed and the extent and type of pooling. The convolution will use a 5x5 kernel and will pad the image with zeros around the edges and use a stride of 1 pixel so that the resulting image (after convolution) has the same size as the original input image. The network will learn the weights for a stack of 32 separate kernels along with 32 bias variables. Finally, after the ReLu is performed the result will be under go 2x2 max pooling, thus halfing both dimensions of the image. The choices for the stride, padding, and pooling are not parameters that the network needs to estimate. Rather these are termed \"hyperparamters\" that are usually set by the network designer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the weight and bias variables for the first convolutional layer as described above. Note the output has depth 32, so there will be 32 feature images after this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike for our softmax regressor above, here we need keep the images as images and not collapse these into vectors; this allows us to perform the 2D convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define are first layer of our CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And wasting no time, we define are second layer. The second layer will have to process 32 feature images coming out of the first layer. Note that the images input to this layer have $\\frac{1}{4}$ the number of pixels as the original input images due to the 2x2 pooling in the previous layer. Note that convolution layer NOT fully connected as our previous hidden layers have been. A unit in the output layer has a limited \"receptive field.\" Its connections to the input layer are spatially limited by the kernel (or filter) size. Also, because of weight sharing in convolutional layers, the number of parameters for a convolutional is the size of the kernel x the depth of the input layer x depth of the output layer + depth of the output layer. So for the second layer of our ConvNet, we have 5 x 5 x 32 x 64 + 64 = 51,264 parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pooling stage of our second convolutional layer, we have 64 7x7 \"feature\" images. In one penultimate fully connected hidden layer, we are going to map these feature imges to a 1024 dimensional feature space. Note we need to flatten these feature images to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is added here, although it is not really needed for such small network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a final linear output layer mapping features to scores topped off with a softmax cross entropy loss function, as explained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we choose an Adam learning rate and update rule. We then run this for 20,000 iterations and evaluate our accuracy after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 1000, training accuracy 0.92\n",
      "step 2000, training accuracy 0.94\n",
      "step 3000, training accuracy 0.94\n",
      "step 4000, training accuracy 0.98\n",
      "step 5000, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 7000, training accuracy 0.98\n",
      "step 8000, training accuracy 0.98\n",
      "step 9000, training accuracy 0.96\n",
      "step 10000, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 12000, training accuracy 0.98\n",
      "step 13000, training accuracy 1\n",
      "step 14000, training accuracy 0.98\n",
      "step 15000, training accuracy 0.98\n",
      "step 16000, training accuracy 0.98\n",
      "step 17000, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "test accuracy 0.9918\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%1000 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an output to compuational graph that computes the label probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_probs = tf.nn.softmax(logits=y_conv, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9918\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we step through some test examples and see how well the network is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e8aa6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Class probabilities =  [[  9.82561144e-09   4.02944625e-06   9.99995232e-01   5.52794063e-07\n",
      "    1.14567238e-10   1.30734061e-11   7.62288344e-13   4.52927162e-09\n",
      "    2.44272286e-07   1.41098175e-10]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0006d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Class probabilities =  [[  1.00000000e+00   9.55037915e-14   1.00204497e-11   2.00734178e-12\n",
      "    3.00243597e-12   3.60783655e-14   1.24180177e-09   1.54670330e-13\n",
      "    7.29696360e-13   2.63964184e-09]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e0cf6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "Class probabilities =  [[  1.78678743e-08   2.13080692e-14   4.30018190e-13   4.07494879e-14\n",
      "    1.43967737e-13   6.06006356e-10   1.00000000e+00   5.61956352e-14\n",
      "    3.06867398e-09   5.18705174e-12]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e12bd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "Class probabilities =  [[  3.71149969e-13   1.06815790e-09   2.73056697e-13   6.85211055e-10\n",
      "    9.96720254e-01   3.12911502e-11   6.12383234e-12   7.73536044e-08\n",
      "    2.03020356e-09   3.27958562e-03]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e308dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  [[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "Class probabilities =  [[  1.98779659e-08   3.16471266e-10   1.29028317e-08   2.70781084e-03\n",
      "    1.94473682e-08   9.44827661e-06   2.06461635e-07   1.81933218e-10\n",
      "    9.97282028e-01   5.10022801e-07]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    batch = mnist.test.next_batch(1)\n",
    "    image = np.asarray(batch[0]).reshape((28, 28))\n",
    "    label = batch[1]\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print (\"Label = \", label)\n",
    "    print (\"Class probabilities = \", y_probs.eval(feed_dict={\n",
    "        x: batch[0], y_: batch[1], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Softmax Regression Model on the MNIST Digits Data, Softmax Multi-Layer Perceptron on the MNIST Digits Data, CNN LeNet\n",
    "\n",
    "- Instructions copied over using PyTorch/torchvision:\n",
    "1) A FULLY commented re-implementation of the networks below using Pytorch.\n",
    "2) your network trained on the same MNIST data as used here.\n",
    "3) an evaluation of the accuracy on the MNIST test set.\n",
    "4) plots of 10 randomly selected digits from the test set along with the correct label and the assigned label.\n",
    "5) have your training record a log of the validation loss and validation accuracy. \n",
    "6) have your training continually save the best model so far (as determined by the validation loss).\n",
    "7) after training, load the saved weights using the best model so far. re-run you accuracy evaluation using these saved weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/33], Train Loss: 0.3836, Val Loss: 0.2554, Val Accuracy: 92.16%\n",
      "Best model saved with validation loss: 0.2554\n",
      "Epoch [2/33], Train Loss: 0.2397, Val Loss: 0.2279, Val Accuracy: 93.56%\n",
      "Best model saved with validation loss: 0.2279\n",
      "Epoch [3/33], Train Loss: 0.2134, Val Loss: 0.2055, Val Accuracy: 94.03%\n",
      "Best model saved with validation loss: 0.2055\n",
      "Epoch [4/33], Train Loss: 0.1967, Val Loss: 0.2029, Val Accuracy: 94.29%\n",
      "Best model saved with validation loss: 0.2029\n",
      "Epoch [5/33], Train Loss: 0.1969, Val Loss: 0.1979, Val Accuracy: 94.41%\n",
      "Best model saved with validation loss: 0.1979\n",
      "Epoch [6/33], Train Loss: 0.1877, Val Loss: 0.1918, Val Accuracy: 95.09%\n",
      "Best model saved with validation loss: 0.1918\n",
      "Epoch [7/33], Train Loss: 0.1870, Val Loss: 0.2879, Val Accuracy: 93.03%\n",
      "Epoch [8/33], Train Loss: 0.1828, Val Loss: 0.2019, Val Accuracy: 94.56%\n",
      "Epoch [9/33], Train Loss: 0.1753, Val Loss: 0.2329, Val Accuracy: 93.75%\n",
      "Epoch [10/33], Train Loss: 0.1785, Val Loss: 0.1687, Val Accuracy: 95.37%\n",
      "Best model saved with validation loss: 0.1687\n",
      "Epoch [11/33], Train Loss: 0.1745, Val Loss: 0.1857, Val Accuracy: 95.39%\n",
      "Epoch [12/33], Train Loss: 0.1683, Val Loss: 0.1896, Val Accuracy: 95.00%\n",
      "Epoch [13/33], Train Loss: 0.1686, Val Loss: 0.1964, Val Accuracy: 95.36%\n",
      "Epoch [14/33], Train Loss: 0.1677, Val Loss: 0.2102, Val Accuracy: 94.95%\n",
      "Epoch [15/33], Train Loss: 0.1645, Val Loss: 0.2054, Val Accuracy: 94.64%\n",
      "Epoch [16/33], Train Loss: 0.1677, Val Loss: 0.2083, Val Accuracy: 94.62%\n",
      "Epoch [17/33], Train Loss: 0.1668, Val Loss: 0.2227, Val Accuracy: 94.46%\n",
      "Epoch [18/33], Train Loss: 0.1697, Val Loss: 0.2089, Val Accuracy: 94.26%\n",
      "Epoch [19/33], Train Loss: 0.1661, Val Loss: 0.1911, Val Accuracy: 95.03%\n",
      "Epoch [20/33], Train Loss: 0.1570, Val Loss: 0.2135, Val Accuracy: 94.93%\n",
      "Epoch [21/33], Train Loss: 0.1573, Val Loss: 0.2049, Val Accuracy: 94.73%\n",
      "Epoch [22/33], Train Loss: 0.1584, Val Loss: 0.3692, Val Accuracy: 91.30%\n",
      "Epoch [23/33], Train Loss: 0.1655, Val Loss: 0.2215, Val Accuracy: 94.49%\n",
      "Epoch [24/33], Train Loss: 0.1518, Val Loss: 0.2535, Val Accuracy: 94.21%\n",
      "Epoch [25/33], Train Loss: 0.1585, Val Loss: 0.3101, Val Accuracy: 92.89%\n",
      "Epoch [26/33], Train Loss: 0.1552, Val Loss: 0.2378, Val Accuracy: 94.45%\n",
      "Epoch [27/33], Train Loss: 0.1579, Val Loss: 0.2302, Val Accuracy: 94.60%\n",
      "Epoch [28/33], Train Loss: 0.1589, Val Loss: 0.2217, Val Accuracy: 94.94%\n",
      "Epoch [29/33], Train Loss: 0.1451, Val Loss: 0.2073, Val Accuracy: 94.96%\n",
      "Epoch [30/33], Train Loss: 0.1588, Val Loss: 0.2192, Val Accuracy: 95.02%\n",
      "Epoch [31/33], Train Loss: 0.1501, Val Loss: 0.1975, Val Accuracy: 95.08%\n",
      "Epoch [32/33], Train Loss: 0.1565, Val Loss: 0.2742, Val Accuracy: 93.78%\n",
      "Epoch [33/33], Train Loss: 0.1536, Val Loss: 0.2470, Val Accuracy: 94.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rosh\\AppData\\Local\\Temp\\ipykernel_8992\\2209008794.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model evaluation - Validation Loss: 0.1687, Validation Accuracy: 95.37%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define data transformations and load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01  # Step size for weight updates\n",
    "batch_size = 100  # Number of images per batch\n",
    "num_iterations = 20000  # Total number of training iterations\n",
    "num_epochs = num_iterations // (len(train_dataset) // batch_size)  # Convert iterations to epochs\n",
    "\n",
    "# Define a Multi-Layer Perceptron model for softmax regression\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the input layer connected to the hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer from input to hidden\n",
    "        self.relu = nn.ReLU()  # ReLU activation function for non-linearity\n",
    "        # Define the hidden layer connected to the output layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Fully connected layer from hidden to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input images\n",
    "        x = self.fc1(x)  # Pass through the first fully connected layer\n",
    "        x = self.relu(x)  # Apply ReLU activation function\n",
    "        x = self.fc2(x)  # Pass through the output layer\n",
    "        return x  # Return raw scores (logits) for each class\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128  # Number of neurons in hidden layer\n",
    "num_classes = 10  # MNIST has 10 classes (digits 0-9)\n",
    "model = MLP(input_size, hidden_size, num_classes)  # Instantiate the model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "\n",
    "# Variables to keep track of the best model\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_model_mlp.pth'\n",
    "\n",
    "# Training log to record metrics\n",
    "training_log = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "# Helper function to evaluate the model on validation data\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            running_val_loss += loss.item()  # Accumulate validation loss\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted classes\n",
    "            total += labels.size(0)  # Count total samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(data_loader)  # Average validation loss\n",
    "    val_accuracy = 100 * correct / total  # Calculate accuracy\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "# Training loop\n",
    "iterations = 0  # Counter for total iterations\n",
    "for epoch in range(num_epochs):  # Run training for the calculated number of epochs\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass: compute model predictions\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  # Calculate loss based on predictions and true labels\n",
    "        \n",
    "        # Backward pass: compute gradients and update weights\n",
    "        optimizer.zero_grad()  # Reset gradients to zero\n",
    "        loss.backward()  # Compute gradients by backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        running_train_loss += loss.item()  # Accumulate training loss\n",
    "        iterations += 1\n",
    "        \n",
    "        # Stop training after reaching the target number of iterations\n",
    "        if iterations >= num_iterations:\n",
    "            break\n",
    "    if iterations >= num_iterations:\n",
    "        break\n",
    "    \n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase: calculate validation loss and accuracy\n",
    "    avg_val_loss, val_accuracy = evaluate_model(model, test_loader)\n",
    "    \n",
    "    # Log metrics for this epoch\n",
    "    training_log['epoch'].append(epoch + 1)\n",
    "    training_log['train_loss'].append(avg_train_loss)\n",
    "    training_log['val_loss'].append(avg_val_loss)\n",
    "    training_log['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save best model weights\n",
    "        print(f\"Best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load the best model weights for evaluation\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_val_loss, val_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Best model evaluation - Validation Loss: {best_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot 10 random images with true and predicted labels\n",
    "def plot_random_predictions(model, test_loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get a batch of test data\n",
    "    images, labels = next(iter(test_loader))\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Select 10 random indices from the batch\n",
    "    random_indices = random.sample(range(len(images)), 10)\n",
    "    \n",
    "    # Plot the images along with true and predicted labels\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        img = images[idx].squeeze()  # Remove unnecessary dimensions\n",
    "        label = labels[idx].item()\n",
    "        pred_label = predicted[idx].item()\n",
    "        \n",
    "        plt.subplot(2, 5, i + 1)  # Arrange plots in a 2x5 grid\n",
    "        plt.imshow(img, cmap='gray')  # Display image in grayscale\n",
    "        plt.title(f'True: {label} | Pred: {pred_label}')\n",
    "        plt.axis('off')  # Hide axes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the model and test_loader\n",
    "plot_random_predictions(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.6084, Val Loss: 0.3931, Val Accuracy: 89.57%\n",
      "Best model saved with validation loss: 0.3931\n",
      "Epoch [2/5], Train Loss: 0.3863, Val Loss: 0.3512, Val Accuracy: 90.09%\n",
      "Best model saved with validation loss: 0.3512\n",
      "Epoch [3/5], Train Loss: 0.3521, Val Loss: 0.3265, Val Accuracy: 90.84%\n",
      "Best model saved with validation loss: 0.3265\n",
      "Epoch [4/5], Train Loss: 0.3346, Val Loss: 0.3129, Val Accuracy: 91.47%\n",
      "Best model saved with validation loss: 0.3129\n",
      "Epoch [5/5], Train Loss: 0.3234, Val Loss: 0.3042, Val Accuracy: 91.42%\n",
      "Best model saved with validation loss: 0.3042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rosh\\AppData\\Local\\Temp\\ipykernel_8992\\124251862.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model on the 10000 test images: 91.42%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Dataset and Dataloader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define Softmax Regression Model\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "model = SoftmaxRegression(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Variables to keep track of the best model\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Training and validation logging\n",
    "training_log = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy for this epoch\n",
    "    avg_val_loss = running_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    \n",
    "    # Logging the metrics for this epoch\n",
    "    training_log['epoch'].append(epoch + 1)\n",
    "    training_log['train_loss'].append(avg_train_loss)\n",
    "    training_log['val_loss'].append(avg_val_loss)\n",
    "    training_log['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model = SoftmaxRegression(input_size, num_classes)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the best model on the 10000 test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [500/20000], Loss: 0.2650\n",
      "Iteration [1000/20000], Loss: 0.1182\n",
      "Iteration [1500/20000], Loss: 0.0759\n",
      "Iteration [2000/20000], Loss: 0.0862\n",
      "Iteration [2500/20000], Loss: 0.0768\n",
      "Iteration [3000/20000], Loss: 0.1809\n",
      "Iteration [3500/20000], Loss: 0.0961\n",
      "Iteration [4000/20000], Loss: 0.0672\n",
      "Iteration [4500/20000], Loss: 0.0331\n",
      "Iteration [5000/20000], Loss: 0.0205\n",
      "Iteration [5500/20000], Loss: 0.0376\n",
      "Iteration [6000/20000], Loss: 0.0727\n",
      "Iteration [6500/20000], Loss: 0.0558\n",
      "Iteration [7000/20000], Loss: 0.0934\n",
      "Iteration [7500/20000], Loss: 0.0439\n",
      "Iteration [8000/20000], Loss: 0.0374\n",
      "Iteration [8500/20000], Loss: 0.0155\n",
      "Iteration [9000/20000], Loss: 0.0442\n",
      "Iteration [9500/20000], Loss: 0.0323\n",
      "Iteration [10000/20000], Loss: 0.0384\n",
      "Iteration [10500/20000], Loss: 0.0063\n",
      "Iteration [11000/20000], Loss: 0.0239\n",
      "Iteration [11500/20000], Loss: 0.0169\n",
      "Iteration [12000/20000], Loss: 0.0160\n",
      "Iteration [12500/20000], Loss: 0.0042\n",
      "Iteration [13000/20000], Loss: 0.0307\n",
      "Iteration [13500/20000], Loss: 0.0037\n",
      "Iteration [14000/20000], Loss: 0.0503\n",
      "Iteration [14500/20000], Loss: 0.0058\n",
      "Iteration [15000/20000], Loss: 0.0500\n",
      "Iteration [15500/20000], Loss: 0.0018\n",
      "Iteration [16000/20000], Loss: 0.0073\n",
      "Iteration [16500/20000], Loss: 0.0764\n",
      "Iteration [17000/20000], Loss: 0.0052\n",
      "Iteration [17500/20000], Loss: 0.1279\n",
      "Iteration [18000/20000], Loss: 0.0477\n",
      "Iteration [18500/20000], Loss: 0.0048\n",
      "Iteration [19000/20000], Loss: 0.0194\n",
      "Iteration [19500/20000], Loss: 0.0010\n",
      "Accuracy of the model on the test set: 97.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001  # Step size for weight updates\n",
    "batch_size = 100  # Number of images per batch\n",
    "num_iterations = 20000  # Total number of training iterations\n",
    "num_epochs = num_iterations // (len(train_dataset) // batch_size)  # Convert to epochs\n",
    "\n",
    "# Define data transformations and load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a Multi-Layer Perceptron model for softmax regression\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the input layer connected to the hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Fully connected layer from input to hidden\n",
    "        self.relu = nn.ReLU()  # ReLU activation function for non-linearity\n",
    "        # Define the hidden layer connected to the output layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Fully connected layer from hidden to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input images\n",
    "        x = self.fc1(x)  # Pass through the first fully connected layer\n",
    "        x = self.relu(x)  # Apply ReLU activation function\n",
    "        x = self.fc2(x)  # Pass through the output layer\n",
    "        return x  # Return raw scores (logits) for each class\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "hidden_size = 128  # Number of neurons in hidden layer\n",
    "num_classes = 10  # MNIST has 10 classes (digits 0-9)\n",
    "model = MLP(input_size, hidden_size, num_classes)  # Instantiate the model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer\n",
    "\n",
    "# Training loop\n",
    "iterations = 0  # Counter for total iterations\n",
    "for epoch in range(num_epochs):  # Run training for the calculated number of epochs\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass: compute model predictions\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  # Calculate loss based on predictions and true labels\n",
    "        \n",
    "        # Backward pass: compute gradients and update weights\n",
    "        optimizer.zero_grad()  # Reset gradients to zero\n",
    "        loss.backward()  # Compute gradients by backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        # Increment iteration count and print progress every 500 iterations\n",
    "        iterations += 1\n",
    "        if iterations % 500 == 0:\n",
    "            print(f\"Iteration [{iterations}/{num_iterations}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Stop training after reaching the target number of iterations\n",
    "        if iterations >= num_iterations:\n",
    "            break\n",
    "    if iterations >= num_iterations:\n",
    "        break\n",
    "\n",
    "# Evaluate the model's accuracy on the test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest predicted score\n",
    "        total += labels.size(0)  # Count total samples\n",
    "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "print(f'Accuracy of the model on the test set: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_random_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/42], Train Loss: 0.4087, Val Loss: 0.2561, Val Accuracy: 92.31%\n",
      "Best model saved with validation loss: 0.2561\n",
      "Epoch [2/42], Train Loss: 0.2092, Val Loss: 0.1674, Val Accuracy: 94.67%\n",
      "Best model saved with validation loss: 0.1674\n",
      "Epoch [3/42], Train Loss: 0.1479, Val Loss: 0.1315, Val Accuracy: 95.91%\n",
      "Best model saved with validation loss: 0.1315\n",
      "Epoch [4/42], Train Loss: 0.1128, Val Loss: 0.1238, Val Accuracy: 96.15%\n",
      "Best model saved with validation loss: 0.1238\n",
      "Epoch [5/42], Train Loss: 0.0928, Val Loss: 0.1157, Val Accuracy: 96.26%\n",
      "Best model saved with validation loss: 0.1157\n",
      "Epoch [6/42], Train Loss: 0.0820, Val Loss: 0.1022, Val Accuracy: 96.74%\n",
      "Best model saved with validation loss: 0.1022\n",
      "Epoch [7/42], Train Loss: 0.0709, Val Loss: 0.0952, Val Accuracy: 97.12%\n",
      "Best model saved with validation loss: 0.0952\n",
      "Epoch [8/42], Train Loss: 0.0618, Val Loss: 0.0835, Val Accuracy: 97.42%\n",
      "Best model saved with validation loss: 0.0835\n",
      "Epoch [9/42], Train Loss: 0.0545, Val Loss: 0.0846, Val Accuracy: 97.36%\n",
      "Epoch [10/42], Train Loss: 0.0493, Val Loss: 0.1108, Val Accuracy: 96.42%\n",
      "Epoch [11/42], Train Loss: 0.0446, Val Loss: 0.1052, Val Accuracy: 96.81%\n",
      "Epoch [12/42], Train Loss: 0.0386, Val Loss: 0.0886, Val Accuracy: 97.33%\n",
      "Epoch [13/42], Train Loss: 0.0380, Val Loss: 0.0970, Val Accuracy: 97.08%\n",
      "Epoch [14/42], Train Loss: 0.0315, Val Loss: 0.0996, Val Accuracy: 97.17%\n",
      "Epoch [15/42], Train Loss: 0.0301, Val Loss: 0.1067, Val Accuracy: 97.08%\n",
      "Epoch [16/42], Train Loss: 0.0258, Val Loss: 0.1032, Val Accuracy: 97.33%\n",
      "Epoch [17/42], Train Loss: 0.0242, Val Loss: 0.1027, Val Accuracy: 97.22%\n",
      "Epoch [18/42], Train Loss: 0.0272, Val Loss: 0.0926, Val Accuracy: 97.35%\n",
      "Epoch [19/42], Train Loss: 0.0190, Val Loss: 0.0922, Val Accuracy: 97.56%\n",
      "Epoch [20/42], Train Loss: 0.0211, Val Loss: 0.0946, Val Accuracy: 97.62%\n",
      "Epoch [21/42], Train Loss: 0.0203, Val Loss: 0.0977, Val Accuracy: 97.48%\n",
      "Epoch [22/42], Train Loss: 0.0167, Val Loss: 0.1110, Val Accuracy: 97.22%\n",
      "Epoch [23/42], Train Loss: 0.0162, Val Loss: 0.1123, Val Accuracy: 97.41%\n",
      "Epoch [24/42], Train Loss: 0.0156, Val Loss: 0.1161, Val Accuracy: 97.23%\n",
      "Epoch [25/42], Train Loss: 0.0151, Val Loss: 0.1010, Val Accuracy: 97.51%\n",
      "Epoch [26/42], Train Loss: 0.0123, Val Loss: 0.1319, Val Accuracy: 96.93%\n",
      "Epoch [27/42], Train Loss: 0.0187, Val Loss: 0.1152, Val Accuracy: 97.31%\n",
      "Epoch [28/42], Train Loss: 0.0126, Val Loss: 0.1333, Val Accuracy: 97.08%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 116\u001b[0m\n\u001b[0;32m    113\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m avg_val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Record metrics in the training log\u001b[39;00m\n\u001b[0;32m    119\u001b[0m training_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 80\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, data_loader)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation for efficiency\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m---> 80\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass to get outputs\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Accumulate weighted loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[29], line 52\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)  \u001b[38;5;66;03m# Pass data through the first layer\u001b[39;00m\n\u001b[0;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)  \u001b[38;5;66;03m# Apply ReLU activation function\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass data through the second layer\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Rosh\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Import neural network modules\n",
    "import torch.optim as optim  # Import optimization algorithms\n",
    "from torchvision import datasets, transforms  # Import dataset and transformations\n",
    "from torch.utils.data import DataLoader, random_split  # For data loading and splitting\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "batch_size = 100  # Number of samples per batch\n",
    "num_iterations = 20000  # Total number of training iterations\n",
    "\n",
    "# Define data transformations: convert images to tensors and normalize pixel values\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize tensors to have mean 0.5 and std 0.5\n",
    "])\n",
    "\n",
    "# Load the full MNIST training dataset\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Split the full training dataset into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_train_dataset))  # Calculate training set size\n",
    "val_size = len(full_train_dataset) - train_size  # Calculate validation set size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])  # Split dataset\n",
    "\n",
    "# Create data loaders for training and validation datasets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)  # Training data loader\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)  # Validation data loader\n",
    "\n",
    "# Load the test dataset (used only for final evaluation)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # Test data loader\n",
    "\n",
    "# Calculate the number of epochs needed to reach the total number of iterations\n",
    "iterations_per_epoch = len(train_loader)  # Number of batches per epoch\n",
    "num_epochs = num_iterations // iterations_per_epoch + 1  # Total number of epochs\n",
    "\n",
    "# Define a Multi-Layer Perceptron (MLP) model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        # Define the first fully connected layer (input to hidden)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Maps input features to hidden layer\n",
    "        self.relu = nn.ReLU()  # Activation function introducing non-linearity\n",
    "        # Define the second fully connected layer (hidden to output)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Maps hidden layer to output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten images from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        x = self.fc1(x)  # Pass data through the first layer\n",
    "        x = self.relu(x)  # Apply ReLU activation function\n",
    "        x = self.fc2(x)  # Pass data through the second layer\n",
    "        return x  # Output raw scores (logits) for each class\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 28 * 28  # Each image has 784 pixels\n",
    "hidden_size = 128  # Increased hidden layer size for better performance\n",
    "num_classes = 10  # Number of classes (digits 0-9)\n",
    "model = MLP(input_size, hidden_size, num_classes)  # Create an instance of the MLP model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam optimizer for training\n",
    "\n",
    "# Variables to keep track of the best model based on validation loss\n",
    "best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "best_model_path = 'best_model_mlp.pth'  # File path to save the best model weights\n",
    "\n",
    "# Dictionary to record training and validation metrics\n",
    "training_log = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "# Helper function to evaluate the model on validation or test data\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    running_loss = 0.0  # Sum of losses over the dataset\n",
    "    correct = 0  # Number of correct predictions\n",
    "    total = 0  # Total number of samples\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)  # Forward pass to get outputs\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate weighted loss\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
    "            total += labels.size(0)  # Increment total samples\n",
    "            correct += (predicted == labels).sum().item()  # Increment correct predictions\n",
    "\n",
    "    avg_loss = running_loss / total  # Calculate average loss over all samples\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy percentage\n",
    "    return avg_loss, accuracy  # Return average loss and accuracy\n",
    "\n",
    "# Training loop\n",
    "iterations = 0  # Counter for total iterations\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0  # Sum of training losses\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass: compute predicted outputs\n",
    "        outputs = model(images)  # Get model predictions\n",
    "        loss = criterion(outputs, labels)  # Compute loss between predictions and true labels\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients from previous step\n",
    "        loss.backward()  # Compute gradients of the loss w.r.t model parameters\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        running_train_loss += loss.item() * images.size(0)  # Accumulate weighted training loss\n",
    "        iterations += 1  # Increment iteration count\n",
    "\n",
    "        # Check if the desired number of iterations is reached\n",
    "        if iterations >= num_iterations:\n",
    "            break  # Exit the batch loop\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    avg_val_loss, val_accuracy = evaluate_model(model, val_loader)\n",
    "\n",
    "    # Record metrics in the training log\n",
    "    training_log['epoch'].append(epoch + 1)\n",
    "    training_log['train_loss'].append(avg_train_loss)\n",
    "    training_log['val_loss'].append(avg_val_loss)\n",
    "    training_log['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    # Print epoch metrics\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the model if it has the best validation loss so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss  # Update best validation loss\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save model weights\n",
    "        print(f\"Best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Break the epoch loop if desired iterations are reached\n",
    "    if iterations >= num_iterations:\n",
    "        break  # Exit the epoch loop\n",
    "\n",
    "# Load the best model weights after training\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Best model evaluation on test set - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
